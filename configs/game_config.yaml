# Game Configuration for Mini Metro RL
experiment_name: "game_settings"
description: "Game-specific configuration options for different maps and difficulties"
tags: ["game", "config", "mini-metro"]

# Game Configuration
game:
  map_name: "paris"
  difficulty: "hard"
  real_time: false
  time_step: 0.5
  max_episode_steps: 5000
  seed: null
  
  # Game mechanics
  week_duration: 45.0
  passenger_spawn_rate: 1.5
  week_escalation_factor: 1.3
  max_station_overload_time: 20.0

# Environment Configuration
environment:
  observation_type: "vector"
  action_space_type: "discrete"
  reward_shaping: true
  normalize_observations: true
  normalize_rewards: true
  
  # Reward weights (aggressive for hard difficulty)
  reward_weights:
    passenger_delivery: 15.0
    passenger_satisfaction: 8.0
    efficiency: 5.0
    resource_optimization: 3.0
    congestion_penalty: -25.0
    passenger_loss_penalty: -30.0
    game_over_penalty: -150.0
    time_bonus: 2.0

# DQN Agent Configuration
dqn:
  learning_rate: 0.0005
  gamma: 0.995
  epsilon_start: 0.9
  epsilon_end: 0.05
  epsilon_decay_steps: 80000
  
  buffer_size: 500000
  batch_size: 128
  min_buffer_size: 25000
  
  target_update_frequency: 5000
  soft_update_tau: 0.01
  
  train_frequency: 2
  gradient_clipping: 5.0
  
  network:
    hidden_sizes: [768, 512, 384, 256]
    activation: "leaky_relu"
    dropout: 0.1
    batch_norm: true
  
  double_dqn: true
  dueling_dqn: true
  noisy_networks: true
  prioritized_replay: true
  n_step: 3

# PPO Agent Configuration
ppo:
  learning_rate: 0.0001
  gamma: 0.995
  gae_lambda: 0.98
  
  clip_range: 0.15
  clip_range_vf: 0.2
  entropy_coef: 0.02
  value_coef: 0.8
  max_grad_norm: 1.0
  
  n_steps: 4096
  batch_size: 128
  n_epochs: 15
  
  network:
    hidden_sizes: [768, 512, 384, 256]
    activation: "leaky_relu"
    dropout: 0.1
    batch_norm: true
  
  normalize_advantage: true
  use_sde: true
  sde_sample_freq: 4

# Training Configuration
training:
  total_timesteps: 2000000
  eval_frequency: 5000
  eval_episodes: 20
  save_frequency: 25000
  
  log_frequency: 500
  tensorboard_log: true
  wandb_log: true
  verbose: 2
  
  save_path: "./models/hard_game"
  checkpoint_frequency: 50000
  keep_n_checkpoints: 10
  
  early_stopping: true
  patience: 20
  min_improvement: 0.05
  
  curriculum_learning: true
  curriculum_stages:
    - stage: 1
      episodes: 1000
      difficulty: "easy"
      max_stations: 5
    - stage: 2
      episodes: 2000
      difficulty: "normal"
      max_stations: 8
    - stage: 3
      episodes: -1
      difficulty: "hard"
      max_stations: 15
  
  hyperopt: true
  hyperopt_trials: 50
  hyperopt_sampler: "tpe"

# Logging Configuration
logging:
  log_level: "DEBUG"
  log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  log_file: "./logs/game_hard.log"
  
  tensorboard_dir: "./logs/tensorboard/hard_game"
  log_graph: true
  log_images: true
  log_histograms: true
  
  wandb_project: "mini-metro-rl"
  wandb_entity: null
  wandb_tags: ["hard", "paris", "curriculum"]
  
  custom_metrics:
    - "passenger_delivery_rate"
    - "network_efficiency"
    - "resource_utilization"
    - "passenger_satisfaction"
    - "station_overload_events"
    - "line_connectivity"

# Visualization Configuration
visualization:
  render_mode: "human"
  fps: 60
  resolution: [1200, 800]
  
  show_passenger_destinations: true
  show_train_loads: true
  show_station_queues: true
  show_performance_metrics: true
  
  record_videos: true
  video_frequency: 50
  video_length: 2000
  
  realtime_plotting: true
  plot_update_frequency: 50

# Multi-agent Configuration
multi_agent:
  agent_type: "ppo"
  num_agents: 5
  shared_experience: true
  centralized_critic: true
  enable_communication: true
  communication_dim: 128
  coordination_graph: true
  value_decomposition: "qmix"